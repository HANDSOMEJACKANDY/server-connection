{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T00:53:46.443074Z",
     "start_time": "2017-12-04T00:53:46.340842Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models.wrappers.fasttext import FastText as FT_wrapper\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import GaussianNoise, BatchNormalization, Dropout\n",
    "from keras.layers import Activation, Input, concatenate, Reshape, merge, dot\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from keras.callbacks import Callback, LambdaCallback, TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from random import shuffle\n",
    "import time\n",
    "import pylab as pl\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T00:50:27.198626Z",
     "start_time": "2017-12-04T00:50:27.153600Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_desired_letter(char):\n",
    "    order = ord(char)\n",
    "    return order >= 97 and order < 123 or order >= 48 and order < 58 or order == ord(\" \") or order == ord(\"'\")\n",
    "\n",
    "\n",
    "def get_text_data():\n",
    "    # load the dataset but only keep the top n words, zero the rest\n",
    "    train_data = pd.read_csv(\"../../input/kickstarter_train.csv\")\n",
    "    # segment all sentences\n",
    "    sent_list = [sent.lower() for text in train_data[\"desc\"] if type(text) is str for sent in sent_tokenize(text)]\n",
    "    # remove symbols in each description\n",
    "    sent_list = [[char for char in sent if is_desired_letter(char)] for sent in sent_list]\n",
    "    sent_list = [''.join(sent).split() for sent in sent_list]\n",
    "    # remove too short desc\n",
    "    train_texts = [sent for sent in sent_list if len(sent) > 3]\n",
    "    train_data = pd.read_csv(\"../../input/kickstarter_test.csv\")\n",
    "    # segment all sentences\n",
    "    sent_list = [sent.lower() for text in train_data[\"desc\"] if type(text) is str for sent in sent_tokenize(text)]\n",
    "    # remove symbols in each description\n",
    "    sent_list = [[char for char in sent if is_desired_letter(char)] for sent in sent_list]\n",
    "    sent_list = [''.join(sent).split() for sent in sent_list]\n",
    "    # remove too short desc\n",
    "    train_texts.extend([sent for sent in sent_list if len(sent) > 3])\n",
    "\n",
    "    return train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T00:50:33.631895Z",
     "start_time": "2017-12-04T00:50:33.619069Z"
    }
   },
   "outputs": [],
   "source": [
    "# get training texts from disk\n",
    "try:\n",
    "    open(\"../../input/all_sentences.txt\", \"r\")\n",
    "except:\n",
    "    print(\"no existing sentence data, generate data from train data\")\n",
    "    # load sentences from training data\n",
    "    train_texts = get_text_data()\n",
    "    print(\"data of length {} grabbed\".format(len(train_texts)))\n",
    "    # write sentences to file\n",
    "    with open(\"../../input/all_sentences.txt\", \"w\") as text_file:\n",
    "        for sent in train_texts:\n",
    "            joint_sent = ' '.join(sent)\n",
    "            text_file.write(joint_sent + '\\n ')\n",
    "    print(\"sentences are written to all_sentences.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A lineSentence must be used to generate training data\n",
    "# Reason remains to be known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T00:50:29.922576Z",
     "start_time": "2017-12-04T00:50:27.247560Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5d7a22fdc141>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mft_home\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m\"/Users/andywu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/fastText/fasttext\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mskipgram_fasttext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFT_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../../input/all_sentences.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skipgram\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mft_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mskipgram_fasttext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w2v_data/fasttext_w2v_64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mskipgram_fasttext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w2v_data/fasttext_w2v_vector_64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram_fasttext_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/gensim/models/wrappers/fasttext.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, ft_path, corpus_file, output_file, model, size, alpha, window, min_count, word_ngrams, loss, sample, negative, iter, min_n, max_n, sorted_vocab, threads)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_fasttext_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_training_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"COMMAND: %s %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft_home =  \"/Users/andywu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/fastText/fasttext\"\n",
    "skipgram_fasttext_model = FT_wrapper.train(corpus_file=\"../../input/all_sentences.txt\", model=\"skipgram\", ft_path=ft_home, sorted_vocab=True, size=64, min_count=15, iter=500, alpha=0.05)\n",
    "skipgram_fasttext_model.save('w2v_data/fasttext_w2v_64')\n",
    "skipgram_fasttext_model.wv.save('w2v_data/fasttext_w2v_vector_64')\n",
    "print(skipgram_fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T00:54:12.532737Z",
     "start_time": "2017-12-04T00:54:12.089033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jr', 0.49340614676475525),\n",
       " ('dragon', 0.4417273700237274),\n",
       " ('bob', 0.4281129539012909),\n",
       " ('jeff', 0.4203910827636719),\n",
       " ('burning', 0.4189816415309906),\n",
       " ('james', 0.41860032081604004),\n",
       " ('nintendo', 0.4155583679676056),\n",
       " ('luther', 0.4138791561126709),\n",
       " ('legendary', 0.41267311573028564),\n",
       " ('murray', 0.4058099389076233)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KeyedVectors.load('w2v_data/fasttext_w2v_128')\n",
    "model.wv.most_similar(positive=['man', 'king'], negative=['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T00:54:25.364907Z",
     "start_time": "2017-12-04T00:54:25.358501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar_to_given(word_list=['you', 'me'], w1='goo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T00:54:36.112906Z",
     "start_time": "2017-12-04T00:54:36.107023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0347137"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['me'].dot(model.wv['me'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T00:54:50.747116Z",
     "start_time": "2017-12-04T00:54:50.741565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'fant' in model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T00:55:12.623209Z",
     "start_time": "2017-12-04T00:55:12.617932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'tic' in model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 412,
   "position": {
    "height": "40px",
    "left": "1311px",
    "right": "19px",
    "top": "105px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
